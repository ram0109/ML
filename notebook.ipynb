{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e36926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rammp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rammp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rammp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ed59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import sklearn\n",
    "## print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0028332",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2817781186.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [3]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install pandas==1.4.2\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## import pandas\n",
    "## print(pandas.__version__)\n",
    "\n",
    "pip install pandas==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5482e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3246517286.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [4]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install flask==1.1.2\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## import flask\n",
    "## print(flask.__version__)\n",
    "\n",
    "pip install flask==1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df934f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1231657891.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install joblib==1.1.0\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## import joblib\n",
    "## print(joblib.__version__)\n",
    "\n",
    "pip install joblib==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54970044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Head:\n",
      "         Al      Cu      Mg        Si      Zn        Zr        Mn        Ag  \\\n",
      "0  0.872024  0.0246  0.0240  0.002498  0.0690  0.001707  0.000778  0.000151   \n",
      "1  0.889151  0.0186  0.0194  0.004803  0.0605  0.001600  0.001313  0.000170   \n",
      "2  0.863341  0.0293  0.0271  0.003928  0.0690  0.001275  0.001809  0.000088   \n",
      "3  0.877796  0.0294  0.0266  0.003395  0.0537  0.001925  0.001598  0.000144   \n",
      "4  0.890138  0.0128  0.0270  0.001624  0.0609  0.000641  0.001710  0.000136   \n",
      "\n",
      "         Li        Ca        Fe        Ti        Sn      Cr        Ge  \\\n",
      "0  0.000093  0.000136  0.003793  0.000567  0.000205  0.0002  0.000140   \n",
      "1  0.000170  0.000171  0.003144  0.000531  0.000199  0.0000  0.000127   \n",
      "2  0.000200  0.000164  0.002238  0.000123  0.000463  0.0007  0.000093   \n",
      "3  0.000055  0.000073  0.004255  0.000407  0.000200  0.0003  0.000051   \n",
      "4  0.000185  0.000072  0.003739  0.000442  0.000209  0.0001  0.000124   \n",
      "\n",
      "         Sc Crack Susceptibility Level  \\\n",
      "0  0.000109                     Medium   \n",
      "1  0.000121                     Medium   \n",
      "2  0.000178                        Low   \n",
      "3  0.000101                     Medium   \n",
      "4  0.000180                     Medium   \n",
      "\n",
      "                                        Observations  \n",
      "0  High Zn can contribute to segregation but stre...  \n",
      "1  High Zn can contribute to segregation but stre...  \n",
      "2  High Zn can contribute to segregation but stre...  \n",
      "3  Mg helps refine grains, reducing cracking tend...  \n",
      "4  High Zn can contribute to segregation but stre...  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 18 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Al                          500 non-null    float64\n",
      " 1   Cu                          500 non-null    float64\n",
      " 2   Mg                          500 non-null    float64\n",
      " 3   Si                          500 non-null    float64\n",
      " 4   Zn                          500 non-null    float64\n",
      " 5   Zr                          500 non-null    float64\n",
      " 6   Mn                          500 non-null    float64\n",
      " 7   Ag                          500 non-null    float64\n",
      " 8   Li                          500 non-null    float64\n",
      " 9   Ca                          500 non-null    float64\n",
      " 10  Fe                          500 non-null    float64\n",
      " 11  Ti                          500 non-null    float64\n",
      " 12  Sn                          500 non-null    float64\n",
      " 13  Cr                          500 non-null    float64\n",
      " 14  Ge                          500 non-null    float64\n",
      " 15  Sc                          500 non-null    float64\n",
      " 16  Crack Susceptibility Level  500 non-null    object \n",
      " 17  Observations                500 non-null    object \n",
      "dtypes: float64(16), object(2)\n",
      "memory usage: 70.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "\n",
    "# Display first few rows to verify data\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf1aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (500, 16)\n",
      "Target shape: (500,)\n",
      "\n",
      "Unique susceptibility levels: ['Medium' 'Low' 'Very Low' 'Very High' 'High']\n"
     ]
    }
   ],
   "source": [
    "# Separate features, target, and observations\n",
    "X = df[['Al', 'Zn', 'Cu', 'Mg', 'Si', 'Zr', 'Mn', 'Ag', 'Li', 'Ca', \n",
    "        'Fe', 'Ti', 'Sn', 'Cr', 'Ge', 'Sc']]\n",
    "y = df['Crack Susceptibility Level']\n",
    "observations = df['Observations']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"\\nUnique susceptibility levels:\", y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8125f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded susceptibility levels: ['High' 'Low' 'Medium' 'Very High' 'Very Low']\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessors\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Scale features and encode target\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Encoded susceptibility levels:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0bb01d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed first observation:\n",
      "Original: High Zn can contribute to segregation but strengthens alloy.\n",
      "Processed: high zn contribute segregation strengthens alloy\n"
     ]
    }
   ],
   "source": [
    "# Initialize text preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "             if token not in stop_words and token.isalnum()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Process observations\n",
    "processed_observations = observations.apply(preprocess_text)\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_observations)\n",
    "\n",
    "print(\"Processed first observation:\")\n",
    "print(\"Original:\", observations.iloc[0])\n",
    "print(\"Processed:\", processed_observations.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9288459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most important elements:\n",
      "   Element  Importance\n",
      "3       Mg    0.212258\n",
      "2       Cu    0.194502\n",
      "1       Zn    0.191161\n",
      "13      Cr    0.179842\n",
      "0       Al    0.139008\n"
     ]
    }
   ],
   "source": [
    "# Train nearest neighbors model\n",
    "nn_model = NearestNeighbors(n_neighbors=3, metric='cosine')\n",
    "nn_model.fit(tfidf_matrix)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
    "rf_model.fit(X_scaled, y_encoded)\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Element': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 5 most important elements:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48456067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_explanation(new_composition):\n",
    "    \"\"\"\n",
    "    Predict crack susceptibility and provide explanation for new composition\n",
    "    \"\"\"\n",
    "    # Scale the input composition\n",
    "    scaled_comp = scaler.transform(new_composition)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_encoded = rf_model.predict(scaled_comp)\n",
    "    pred_label = label_encoder.inverse_transform(pred_encoded)\n",
    "    prob_scores = rf_model.predict_proba(scaled_comp)\n",
    "    \n",
    "    # Find similar cases\n",
    "    query_vec = vectorizer.transform(processed_observations)\n",
    "    distances, indices = nn_model.kneighbors(query_vec)\n",
    "    similar_cases_idx = indices[0]\n",
    "    \n",
    "    # Generate explanation\n",
    "    explanation = \"Prediction Analysis:\\n\"\n",
    "    explanation += f\"1. Predicted Crack Susceptibility: {pred_label[0]}\\n\\n\"\n",
    "    \n",
    "    # Add probability scores\n",
    "    explanation += \"2. Confidence Levels:\\n\"\n",
    "    for i, label in enumerate(label_encoder.classes_):\n",
    "        explanation += f\"   {label}: {prob_scores[0][i]:.2f}\\n\"\n",
    "    \n",
    "    # Add composition-based reasoning\n",
    "    explanation += \"\\n3. Key Influencing Elements:\\n\"\n",
    "    top_elements = feature_importance.head(3)\n",
    "    for _, row in top_elements.iterrows():\n",
    "        element = row['Element']\n",
    "        importance = row['Importance']\n",
    "        value = new_composition[element].values[0]\n",
    "        explanation += f\"   - {element}: {value:.2f}% (Importance: {importance:.3f})\\n\"\n",
    "    \n",
    "    # Add similar cases\n",
    "    explanation += \"\\n4. Similar Historical Cases:\\n\"\n",
    "    for i, idx in enumerate(similar_cases_idx[:3], 1):\n",
    "        explanation += f\"Case {i}:\\n\"\n",
    "        explanation += f\"   Susceptibility: {y.iloc[idx]}\\n\"\n",
    "        explanation += f\"   Observation: {observations.iloc[idx]}\\n\\n\"\n",
    "    \n",
    "    return pred_label[0], explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db289e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Crack Susceptibility Analysis ===\n",
      "Prediction Analysis:\n",
      "1. Predicted Crack Susceptibility: Very Low\n",
      "\n",
      "2. Confidence Levels:\n",
      "   High: 0.12\n",
      "   Low: 0.08\n",
      "   Medium: 0.03\n",
      "   Very High: 0.24\n",
      "   Very Low: 0.53\n",
      "\n",
      "3. Key Influencing Elements:\n",
      "   - Mg: 2.00% (Importance: 0.212)\n",
      "   - Cu: 1.50% (Importance: 0.195)\n",
      "   - Zn: 2.50% (Importance: 0.191)\n",
      "\n",
      "4. Similar Historical Cases:\n",
      "Case 1:\n",
      "   Susceptibility: Medium\n",
      "   Observation: High Zn can contribute to segregation but strengthens alloy.\n",
      "\n",
      "Case 2:\n",
      "   Susceptibility: Medium\n",
      "   Observation: High Zn can contribute to segregation but strengthens alloy.\n",
      "\n",
      "Case 3:\n",
      "   Susceptibility: Medium\n",
      "   Observation: High Zn can contribute to segregation but strengthens alloy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a test composition\n",
    "new_composition = pd.DataFrame({\n",
    "    'Al': [92.5], 'Zn': [2.5], 'Cu': [1.5], 'Mg': [2.0],\n",
    "    'Si': [0.2], 'Zr': [0.1], 'Mn': [0.3], 'Ag': [0.1],\n",
    "    'Li': [0.1], 'Ca': [0.1], 'Fe': [0.2], 'Ti': [0.1],\n",
    "    'Sn': [0.1], 'Cr': [0.1], 'Ge': [0.1], 'Sc': [0.1]\n",
    "})\n",
    "\n",
    "# Get prediction and explanation\n",
    "prediction, explanation = predict_with_explanation(new_composition)\n",
    "\n",
    "print(\"=== Crack Susceptibility Analysis ===\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea73cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and preprocessors saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained models\n",
    "joblib.dump(rf_model, 'rf_model.pkl')  # RandomForest model\n",
    "joblib.dump(scaler, 'scaler.pkl')  # Standard Scaler\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')  # Label Encoder\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')  # TF-IDF Vectorizer\n",
    "joblib.dump(nn_model, 'nn_model.pkl')  # Nearest Neighbors Model\n",
    "\n",
    "print(\"Models and preprocessors saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d27e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rammp\\Documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f556750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
